{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data with PyLiPD and Pyleoclim\n",
    "\n",
    "This notebook lays out the details of how we load data from LiPD files using [PyLiPD](https://pylipd.readthedocs.io/en/latest/) into [Pyleoclim](https://pyleoclim-util.readthedocs.io/en/latest/) objects that we use for the remainder of our analysis. We also do a little bit of pre-processing for plotting that will be used later in this book.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "\n",
    "1. Load records with PyLiPD\n",
    "2. Process data into a [MultipleGeoSeries](https://pyleoclim-util.readthedocs.io/en/latest/core/api.html#multiplegeoseries-pyleoclim-multiplegeoseries) object\n",
    "3. Use Pyleoclim to create new objects that will plot more nicely than the raw records for later Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary pacakges\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pyleoclim as pyleo\n",
    "from pylipd.lipd import LiPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load our records into a [LiPD](https://pylipd.readthedocs.io/en/latest/api.html#lipd-pylipd-lipd-lipd) object to identify the record names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 15 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████▌                                                                              | 1/15 [00:00<00:07,  1.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 25.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    }
   ],
   "source": [
    "# Loading files into pylipd object\n",
    "\n",
    "lipd_path = '../../data/speleothems/'\n",
    "\n",
    "all_files = LiPD()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    all_files.load_from_dir(lipd_path,parallel=True)\n",
    "\n",
    "record_names = all_files.get_all_dataset_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we parse through the records and load each into a LiPD object individually. These we convert into pandas [DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), which are straightforward to work with. Once we've extracted the relevant info from each dataframe, we create a [GeoSeries](https://pyleoclim-util.readthedocs.io/en/latest/core/api.html#geoseries-pyleoclim-geoseries) object, which we add to a list for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 126.55it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n",
      "NaNs have been detected and dropped.\n",
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: DonggeD4.China.2004 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 161.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n",
      "NaNs have been detected and dropped.\n",
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: DonggeD3.China.2004 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 159.52it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n",
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: Botuvera.Brazil.2005 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 217.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n",
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: Linzhu.China.2009 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 97.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n",
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: Sanbao.China.2016 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 130.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: Leviathan.Nevada.2017 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 139.14it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: Clearwater.Borneo.2016 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 157.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time axis values sorted in ascending order\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting timeseries from dataset: Peqiin.Israel.2003 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 120.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n",
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: BuckeyeCreek.WestVirginia.2019 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 198.57it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n",
      "Time axis values sorted in ascending order\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting timeseries from dataset: JerusalemWest.Jerusalem.1999 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 101.09it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: Bittoo.India.2016 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 181.95it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: Kesang.China.2012 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 162.50it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs have been detected and dropped.\n",
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: CuevadelDiamante.Peru.2013 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 127.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: DevilsHole.Nevada.2017 ...\n",
      "Loading 1 LiPD files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                             | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 152.85it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time axis values sorted in ascending order\n",
      "Extracting timeseries from dataset: Soreq.Israel.2003 ...\n"
     ]
    }
   ],
   "source": [
    "# Parsing this object for oxygen isotopes and saving them into a dictionary\n",
    "\n",
    "series_list = []\n",
    "doi_dict = {}\n",
    "lat_dict = {}\n",
    "value_name = 'd18O'\n",
    "\n",
    "for record in record_names:\n",
    "    d = LiPD()\n",
    "    d.load(f'{lipd_path}/{record}.lpd')\n",
    "    df = d.get_timeseries_essentials()\n",
    "    row = df[df['paleoData_variableName']==value_name]\n",
    "    lat = row['geo_meanLat'].to_numpy()[0]\n",
    "    lon = row['geo_meanLon'].to_numpy()[0]\n",
    "    elevation = row['geo_meanElev'].to_numpy()[0]\n",
    "    value = row['paleoData_values'].to_numpy()[0]\n",
    "    value_unit = row['paleoData_units'].to_numpy()[0]\n",
    "    time = row['time_values'].to_numpy()[0]\n",
    "    time_unit = row['time_units'].to_numpy()[0]\n",
    "    time_name = row['time_variableName'].to_numpy()[0]\n",
    "    label = row['dataSetName'].to_numpy()[0]\n",
    "    geo_series = pyleo.GeoSeries(time=time,\n",
    "                                 value=value,\n",
    "                                 lat=lat,\n",
    "                                 lon=lon,\n",
    "                                 elevation=elevation,\n",
    "                                 time_unit=time_unit,\n",
    "                                 time_name=time_name,\n",
    "                                 value_name=value_name,\n",
    "                                 value_unit=value_unit,\n",
    "                                 label=label,\n",
    "                                 archiveType='speleothem')\n",
    "    series_list.append(geo_series)\n",
    "\n",
    "    _,doi_df = d.get_timeseries(dsnames=record,to_dataframe=True)\n",
    "    doi_dict[label] = doi_df['pub1_doi'][0]\n",
    "    lat_dict[label] = lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One record requires special attention, as it is a composite of two different records. We handle that here, and creat a dictionary with record names as the keys and GeoSeries objects as the values for later use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time axis values sorted in ascending order\n"
     ]
    }
   ],
   "source": [
    "# Handling dongge separately due to the presence of two records\n",
    "\n",
    "series_dicts = []\n",
    "geo_ms_composite_list = []\n",
    "\n",
    "for series in series_list:\n",
    "    if 'dongge' in series.label.lower():\n",
    "        time = series.time\n",
    "        value = series.value\n",
    "        tv_dict = {t:value[idx] for idx,t in enumerate(time)}\n",
    "        series_dicts.append(tv_dict)\n",
    "        lat=series.lat\n",
    "        lon=series.lon\n",
    "        elevation=series.elevation\n",
    "        time_unit=series.time_unit\n",
    "        time_name=series.time_name\n",
    "        value_name=series.value_name\n",
    "        value_unit=series.value_unit\n",
    "    else:\n",
    "        geo_ms_composite_list.append(series)\n",
    "\n",
    "series1_dict = series_dicts[0]\n",
    "series2_dict = series_dicts[1]\n",
    "\n",
    "combined_timestamps = set(series1_dict.keys()).union(set(series2_dict.keys()))\n",
    "\n",
    "# Combine the values for overlapping timestamps\n",
    "combined_series = []\n",
    "for timestamp in sorted(combined_timestamps):\n",
    "    value1 = series1_dict.get(timestamp)\n",
    "    value2 = series2_dict.get(timestamp)\n",
    "    \n",
    "    if value1 is not None:\n",
    "        combined_series.append((timestamp, value1))\n",
    "    elif value2 is not None:\n",
    "        combined_series.append((timestamp, value2))\n",
    "\n",
    "dongge_composite = pyleo.GeoSeries(time=[tv[0] for tv in combined_series],\n",
    "                                   value=[tv[1] for tv in combined_series],\n",
    "                                   lat=lat,\n",
    "                                   lon=lon,\n",
    "                                   elevation=elevation,\n",
    "                                   time_unit=time_unit,\n",
    "                                   time_name=time_name,\n",
    "                                   value_name=value_name,\n",
    "                                   value_unit=value_unit,\n",
    "                                   archiveType='speleothem',\n",
    "                                   label='Dongge.China.2004')\n",
    "\n",
    "geo_ms_composite_list.append(dongge_composite)\n",
    "geo_ms_composites = pyleo.MultipleGeoSeries(geo_ms_composite_list)\n",
    "geo_ms_composite_dict = {series.label:series for series in geo_ms_composite_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create versions of each GeoSeries that are useful for plotting. These objects won't be used to do any analysis, the raw records will do that, but the plotting objects are structured so that we don't plot lines over hiatuses. We'll create an unsmoothed and a smoothed version of each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|████████████████████████████████████████████████████████████████▍                 | 11/14 [00:00<00:00, 108.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:00<00:00, 115.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating series that can be plotted nicely.\n",
    "\n",
    "plotting_series_dict = {}\n",
    "smooth_plotting_series_dict = {}\n",
    "\n",
    "for series in tqdm(geo_ms_composite_dict.values()):\n",
    "    series.value_name = r'$\\delta^{18}O$'\n",
    "    series.value_unit = u'‰'\n",
    "    series = series.convert_time_unit('kyr BP')\n",
    "    segments = series.segment(factor=15)\n",
    "    if isinstance(segments,pyleo.core.multiplegeoseries.MultipleGeoSeries):\n",
    "        plotting_series_value = []\n",
    "        plotting_series_time = []\n",
    "        smooth_plotting_series_value = []\n",
    "        smooth_plotting_series_time = []\n",
    "        for segment in segments.series_list:\n",
    "            if max(segment.time)-min(segment.time) > 12:\n",
    "                plotting_series_value.extend(segment.value)\n",
    "                plotting_series_value.append(np.nan)\n",
    "                plotting_series_time.extend(segment.time)\n",
    "                plotting_series_time.append(plotting_series_time[-1]+.0000001)\n",
    "\n",
    "                smooth_segment = segment.interp().filter(cutoff_scale=6)\n",
    "                smooth_plotting_series_value.extend(smooth_segment.value)\n",
    "                smooth_plotting_series_value.append(np.nan)\n",
    "                smooth_plotting_series_time.extend(smooth_segment.time)\n",
    "                smooth_plotting_series_time.append(smooth_plotting_series_time[-1]+.0000001)\n",
    "\n",
    "        plotting_series = series.copy()\n",
    "        plotting_series.value = np.array(plotting_series_value)\n",
    "        plotting_series.time = np.array(plotting_series_time)\n",
    "\n",
    "        smooth_plotting_series = series.copy()\n",
    "        smooth_plotting_series.value = np.array(smooth_plotting_series_value)\n",
    "        smooth_plotting_series.time = np.array(smooth_plotting_series_time)\n",
    "\n",
    "    else:\n",
    "        plotting_series = series\n",
    "        smooth_plotting_series = series.interp().filter(cutoff_scale=6)\n",
    "\n",
    "    plotting_series_dict[series.label] = plotting_series\n",
    "    smooth_plotting_series_dict[series.label] = smooth_plotting_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save these as pickle files for easy loading later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the output as a pickle file\n",
    "\n",
    "with open('../../data/geo_ms_composite_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(geo_ms_composite_dict, f)\n",
    "\n",
    "with open('../../data/plotting_series_dict.pkl','wb') as handle:\n",
    "    pickle.dump(plotting_series_dict,handle)\n",
    "\n",
    "with open('../../data/smooth_plotting_series_dict.pkl','wb') as handle:\n",
    "    pickle.dump(smooth_plotting_series_dict,handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inso_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}